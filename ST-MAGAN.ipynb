{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9c8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import distance\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8483a2e",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc67f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>region</th>\n",
       "      <th>time_delta</th>\n",
       "      <th>dist_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6138</td>\n",
       "      <td>2010-09-04 19:06:46</td>\n",
       "      <td>30.528386</td>\n",
       "      <td>-97.810511</td>\n",
       "      <td>39965</td>\n",
       "      <td>9v6mw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6138</td>\n",
       "      <td>2010-09-08 14:16:54</td>\n",
       "      <td>30.454928</td>\n",
       "      <td>-97.795663</td>\n",
       "      <td>1412780</td>\n",
       "      <td>9v6mq</td>\n",
       "      <td>328208.0</td>\n",
       "      <td>8267.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6138</td>\n",
       "      <td>2010-09-12 22:04:41</td>\n",
       "      <td>30.335013</td>\n",
       "      <td>-97.706323</td>\n",
       "      <td>2020206</td>\n",
       "      <td>9v6s8</td>\n",
       "      <td>373667.0</td>\n",
       "      <td>15825.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6138</td>\n",
       "      <td>2010-09-14 15:14:46</td>\n",
       "      <td>30.407691</td>\n",
       "      <td>-97.713379</td>\n",
       "      <td>26919</td>\n",
       "      <td>9v6sb</td>\n",
       "      <td>148205.0</td>\n",
       "      <td>8085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6138</td>\n",
       "      <td>2010-09-14 15:14:59</td>\n",
       "      <td>30.407769</td>\n",
       "      <td>-97.713478</td>\n",
       "      <td>667822</td>\n",
       "      <td>9v6sb</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16459</th>\n",
       "      <td>90112</td>\n",
       "      <td>2010-08-01 19:43:04</td>\n",
       "      <td>30.286428</td>\n",
       "      <td>-97.741806</td>\n",
       "      <td>419988</td>\n",
       "      <td>9v6kr</td>\n",
       "      <td>325.0</td>\n",
       "      <td>81.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16460</th>\n",
       "      <td>90112</td>\n",
       "      <td>2010-08-01 19:43:14</td>\n",
       "      <td>30.286082</td>\n",
       "      <td>-97.741901</td>\n",
       "      <td>10532</td>\n",
       "      <td>9v6kr</td>\n",
       "      <td>10.0</td>\n",
       "      <td>39.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16461</th>\n",
       "      <td>90112</td>\n",
       "      <td>2010-08-01 20:37:34</td>\n",
       "      <td>30.270734</td>\n",
       "      <td>-97.753703</td>\n",
       "      <td>9241</td>\n",
       "      <td>9v6kp</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>2045.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16462</th>\n",
       "      <td>90112</td>\n",
       "      <td>2010-09-12 21:04:31</td>\n",
       "      <td>30.270734</td>\n",
       "      <td>-97.753703</td>\n",
       "      <td>9241</td>\n",
       "      <td>9v6kp</td>\n",
       "      <td>3630417.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16463</th>\n",
       "      <td>90112</td>\n",
       "      <td>2010-10-08 16:48:50</td>\n",
       "      <td>30.244443</td>\n",
       "      <td>-97.751949</td>\n",
       "      <td>13299</td>\n",
       "      <td>9v6kp</td>\n",
       "      <td>2231059.0</td>\n",
       "      <td>2919.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16464 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user                time   latitude  longitude  location region  \\\n",
       "0       6138 2010-09-04 19:06:46  30.528386 -97.810511     39965  9v6mw   \n",
       "1       6138 2010-09-08 14:16:54  30.454928 -97.795663   1412780  9v6mq   \n",
       "2       6138 2010-09-12 22:04:41  30.335013 -97.706323   2020206  9v6s8   \n",
       "3       6138 2010-09-14 15:14:46  30.407691 -97.713379     26919  9v6sb   \n",
       "4       6138 2010-09-14 15:14:59  30.407769 -97.713478    667822  9v6sb   \n",
       "...      ...                 ...        ...        ...       ...    ...   \n",
       "16459  90112 2010-08-01 19:43:04  30.286428 -97.741806    419988  9v6kr   \n",
       "16460  90112 2010-08-01 19:43:14  30.286082 -97.741901     10532  9v6kr   \n",
       "16461  90112 2010-08-01 20:37:34  30.270734 -97.753703      9241  9v6kp   \n",
       "16462  90112 2010-09-12 21:04:31  30.270734 -97.753703      9241  9v6kp   \n",
       "16463  90112 2010-10-08 16:48:50  30.244443 -97.751949     13299  9v6kp   \n",
       "\n",
       "       time_delta  dist_delta  \n",
       "0             0.0        0.00  \n",
       "1        328208.0     8267.49  \n",
       "2        373667.0    15825.25  \n",
       "3        148205.0     8085.58  \n",
       "4            13.0       12.88  \n",
       "...           ...         ...  \n",
       "16459       325.0       81.35  \n",
       "16460        10.0       39.49  \n",
       "16461      3260.0     2045.54  \n",
       "16462   3630417.0        0.00  \n",
       "16463   2231059.0     2919.44  \n",
       "\n",
       "[16464 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./modeldata/gowalla/\"\n",
    "train_data = pd.read_csv(path+\"train_data.csv\",\n",
    "                         parse_dates=['time'], infer_datetime_format=True)\n",
    "test_data = pd.read_csv(path+\"test_data.csv\",\n",
    "                        parse_dates=['time'], infer_datetime_format=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cc583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21932, 100)\n",
      "POI_vec_dim =  100\n",
      "16054\n",
      "16054\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "# get poi_embedding vector, word2idx, idx2word\n",
    "loc_embedding = np.load(path+'locationembedding.npy')\n",
    "_,POI_vec_dim = loc_embedding.shape\n",
    "print(loc_embedding.shape)\n",
    "print(\"POI_vec_dim = \",POI_vec_dim)\n",
    "\n",
    "with open(path+'locationidx2node.pickle', 'rb') as f:\n",
    "    loc_idx2node = pickle.load(f)\n",
    "print(len(loc_idx2node))\n",
    "with open(path+'locationnode2idx.pickle', 'rb') as f:\n",
    "    loc_node2idx = pickle.load(f)\n",
    "\n",
    "num_POI = len(loc_node2idx)\n",
    "print(num_POI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce86dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 50)\n",
      "region_vec_dim =  50\n",
      "594\n",
      "594\n"
     ]
    }
   ],
   "source": [
    "# get region_embedding vector, word2idx, idx2word\n",
    "\n",
    "reg_embedding = np.load(path+'regionembedding.npy')\n",
    "_,region_vec_dim = reg_embedding.shape\n",
    "print(reg_embedding.shape)\n",
    "print(\"region_vec_dim = \",region_vec_dim)\n",
    "\n",
    "with open(path+'regionidx2node.pickle', 'rb') as f:\n",
    "    reg_idx2node = pickle.load(f)\n",
    "print(len(reg_idx2node))\n",
    "with open(path+'regionnode2idx.pickle', 'rb') as f:\n",
    "    reg_node2idx = pickle.load(f)\n",
    "print(len(reg_node2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81267635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2182179  0.2182179  0.21952853 0.         0.10976426 0.10783277\n",
      " 0.10910895 0.23094012 0.10846522 0.10910895 0.1118034  0.\n",
      " 0.1118034  0.22792114 0.10783277 0.         0.         0.10976427\n",
      " 0.11547006 0.10721125 0.1118034  0.         0.         0.\n",
      " 0.         0.         0.         0.10783277 0.         0.\n",
      " 0.10846523 0.         0.         0.1125088  0.         0.\n",
      " 0.         0.11111111 0.11547006 0.         0.         0.\n",
      " 0.         0.10910895 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1125088  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.10783277 0.2264554  0.         0.         0.10910895\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.22501759 0.         0.         0.\n",
      " 0.11952286 0.         0.         0.         0.         0.\n",
      " 0.11952286 0.         0.         0.         0.         0.\n",
      " 0.2264554  0.         0.         0.         0.         0.\n",
      " 0.         0.10721126 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.10846523 0.\n",
      " 0.         0.         0.         0.13736057 0.         0.\n",
      " 0.         0.         0.         0.10910895 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.11704115\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12126781 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.10846523 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.12038586 0.         0.         0.\n",
      " 0.         0.         0.12216944 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.12038585 0.         0.11867817 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.12126782 0.125      0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.125      0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.125      0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14744195 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.1230915\n",
      " 0.         0.         0.         0.         0.         0.13867505\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.13483997\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.13018891 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13867505 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.12403473 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13483997 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.12803689 0.14433756 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14744195 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "(594, 594)\n"
     ]
    }
   ],
   "source": [
    "# get matrix of weight\n",
    "dist_matrix = np.load(path+\"dist_matrix.npy\")\n",
    "print(dist_matrix[3])\n",
    "visit_matrix = np.load(path+\"visit_matrix.npy\")\n",
    "print(visit_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdd2d2",
   "metadata": {},
   "source": [
    "# GCN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7a4a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=594, num_edges=16202,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={'ew': Scheme(shape=(1,), dtype=torch.float16)})\n",
      "Graph(num_nodes=594, num_edges=12738,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={'ew': Scheme(shape=(1,), dtype=torch.float16)})\n",
      "tensor([0.2222], dtype=torch.float16)\n",
      "tensor([0.0815], dtype=torch.float16)\n",
      "cpu\n",
      "Wall time: 652 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get graphs\n",
    "from graphbuild import build_graph\n",
    "dist_graph = build_graph(dist_matrix)\n",
    "visit_graph = build_graph(visit_matrix)\n",
    "\n",
    "print(dist_graph)\n",
    "print(visit_graph)\n",
    "print(dist_graph.edata['ew'][2])\n",
    "print(visit_graph.edata['ew'][2])\n",
    "# use torch.float32 in the whole model\n",
    "print(dist_graph.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ecd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph convolution\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_g, out_g, hid_dim, num_layers, pred_len):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.hidden_g = hidden_g\n",
    "        self.out_g = out_g\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.pred_len = pred_len\n",
    "        self.conv1 = GATConv(in_feats = self.in_feats,\n",
    "                               out_feats = self.hidden_g,\n",
    "                               num_heads =1,\n",
    "                              activation=nn.ReLU())\n",
    "        self.conv2 = GATConv(in_feats = self.hidden_g,\n",
    "                               out_feats = self.hidden_g,\n",
    "                               num_heads =1,\n",
    "                              activation=nn.ReLU())\n",
    "        self.conv3 = GATConv(in_feats = self.hidden_g,\n",
    "                               out_feats = self.out_g,\n",
    "                               num_heads =1,\n",
    "                              activation=nn.Tanh())\n",
    "        self.GRU = nn.GRU(input_size = self.out_g*self.pred_len,\n",
    "                          hidden_size = self.hid_dim,\n",
    "                          num_layers = num_layers)\n",
    "    def forward(self, g, nodes_vec, inputs,device):\n",
    "        \"\"\"\n",
    "        g: graph\n",
    "        nodes_vec: (node_num, in_feats);feature vector of each region, as well as region_embedding\n",
    "        inputs: (seq_len, batch, pred_len);sequence of train region(id)\n",
    "        \"\"\"\n",
    "        h = self.conv1(g, nodes_vec)\n",
    "        h = self.conv2(g, h)\n",
    "        h = self.conv3(g, h)\n",
    "        # h:[node_num, out_g]\n",
    "        a,b,c = inputs.shape\n",
    "        inputs = inputs.reshape(-1, c).long()\n",
    "        l = torch.zeros(a*b, c*self.out_g).to(device).half()\n",
    "        for i in range(a*b):\n",
    "            l[i] = h[inputs[i],:].reshape(1,-1)\n",
    "        l = l.reshape(a,b,c*self.out_g)\n",
    "        _,h_v = self.GRU(l)\n",
    "        # h_v :[layers, batch, hid_dim]\n",
    "        return h_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8f622",
   "metadata": {},
   "source": [
    "# encoder-decoder module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932a03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from STLSTM import STLSTM\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_1, hid_dim, n_layers, pred_len, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_1 = hidden_1\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.w_t = nn.Parameter(torch.rand(pred_len, pred_len*102))\n",
    "        self.w_s = nn.Parameter(torch.rand(pred_len, pred_len*102))\n",
    "        # input_dim = 100*pred_len\n",
    "        self.LSTM1 = STLSTM(self.input_dim, self.hidden_1)\n",
    "        self.LSTM2 = nn.LSTM(input_size = self.hidden_1,\n",
    "                             hidden_size = self.hid_dim,\n",
    "                             num_layers = self.n_layers,\n",
    "                             dropout = self.dropout)\n",
    "    \n",
    "    def forward(self, input_l, input_s, input_q):\n",
    "        input_q = torch.matmul(input_q, self.w_t)\n",
    "        input_s = torch.matmul(input_s, self.w_s)\n",
    "        # input_q,s,l =[seq_len, batch, pred_len*100]\n",
    "        out,_ = self.LSTM1(input_l, input_s, input_q)\n",
    "        outputs, (hidden, cell) = self.LSTM2(out)\n",
    "        #print(\"hidden.shape = \",hidden.shape)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac109dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid_d, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_d = hid_d\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.LSTM3 = nn.LSTM(input_size = self.in_dim,\n",
    "                             hidden_size = self.hid_d,\n",
    "                             num_layers = self.n_layers,\n",
    "                             dropout = self.dropout)\n",
    "        self.out = nn.Linear(self.hid_d, self.output_dim)\n",
    "        \n",
    "    def forward(self, input_dec, hidden, cell):\n",
    "        # hidden,cell : [layers, batch, hid_1]\n",
    "        # input_dec : [seq_len, batch, in_dim]\n",
    "        #print(\"input_dec.shape = \",input_dec.shape)\n",
    "        #print(\"hidden.shape = \",hidden.shape)\n",
    "        #print(\"cell.shape = \",cell.shape)\n",
    "        output, (hidden, cell) = self.LSTM3(input_dec, (hidden, cell))\n",
    "        # output : [seq_len, batch, hid_1]\n",
    "        # hidden,cell : [layers, batch, hid_1]\n",
    "        output = self.out(output)\n",
    "        # output : [seq_len, batch, output_dim]\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20fbda",
   "metadata": {},
   "source": [
    "# ST-MAGCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b03ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STMAGCN(nn.Module):\n",
    "    def __init__(self, encoder, decoder, gcn_dist, gcn_visit, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.gcn_dist = gcn_dist\n",
    "        self.gcn_visit = gcn_visit\n",
    "        self.device = device\n",
    "        hid_dim = self.encoder.hid_dim\n",
    "        hid_d = self.decoder.hid_d\n",
    "        self.trans = nn.Linear(hid_dim*3, hid_d)\n",
    "        \n",
    "    def forward(self, poi_embedding, input_l, input_s, input_q, trg, input_0,\n",
    "                dist_graph,visit_graph, inputs, nodes_vec, teacher_forcing_ratio=0.6):\n",
    "        # trg : [seq_len, batch, max_len], max_len=4\n",
    "        # input_l : [seq_len, batch, pred_len*100]\n",
    "        # input_q,input_s : [seq_len, batch, pred_len]\n",
    "        trg_n, batch_size, max_len = trg.shape\n",
    "        output_dim = self.decoder.output_dim\n",
    "        hidden_dim = self.encoder.hid_dim\n",
    "        outputs = torch.zeros(trg_n, batch_size, max_len*output_dim).half().to(self.device)\n",
    "        hidden, cell = self.encoder.forward(input_l, input_s, input_q)\n",
    "        # hidden,cell : [layers, batch, hid_dim]\n",
    "        h_dist = gcn_dist(dist_graph, nodes_vec, inputs, self.device)\n",
    "        h_visit = gcn_visit(visit_graph, nodes_vec, inputs, self.device)\n",
    "        # h_dist/h_visit :[layers, batch, hid_dim]\n",
    "        hidden = torch.cat([hidden, h_dist, h_visit], dim=2)\n",
    "        cell = torch.cat([cell, h_dist, h_visit], dim=2)\n",
    "        # hidden,cell:[layers, batch, hid_dim*3]\n",
    "        hidden = self.trans(hidden)\n",
    "        cell = self.trans(cell)\n",
    "        # hidden,cell : [layers, batch, hid_d]\n",
    "        input = input_0.to(self.device)  # [seq_len, batch_size, in_dim]\n",
    "        \n",
    "        for t in range(0, max_len):\n",
    "            output, hidden, cell = self.decoder.forward(input, hidden, hidden)\n",
    "            # hidden,cell : [layers, batch, hid_1]\n",
    "            # output : [seq_len, batch, output_dim]\n",
    "            ind = [0,output_dim,output_dim*2,output_dim*3,output_dim*4]\n",
    "            outputs[:,:,ind[t]:ind[t+1]] = output\n",
    "            if t ==3:\n",
    "                break\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2).unsqueeze(2)\n",
    "            # top1 : [seq_len, batch, 1]\n",
    "            top1 = top1.reshape(-1)  # seq_len*batch\n",
    "            top1 = torch.tensor([poi_embedding[int(i)].tolist() for i in top1.tolist()])\n",
    "            top1 = top1.reshape(-1,batch_size,100).half().to(self.device)\n",
    "            real = trg[:,:,t]   #[seq_len, batch, 1]\n",
    "            real = real.reshape(-1)  # seq_len*batch\n",
    "            real = np.array([poi_embedding[int(i)].tolist() for i in real.tolist()])\n",
    "            real = torch.from_numpy(real)\n",
    "            real = real.reshape(-1,batch_size,100).half().to(self.device)\n",
    "            #top1,real : [seq_len, batch, embed_dim] embed_dim=100=in_dim\n",
    "            input = real if teacher_force else top1\n",
    "        outputs = outputs.view(-1, output_dim)\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        outputs = outputs.half()\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c6af2",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fc1d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2256, 2448)\n",
      "train_input_l.shape =  torch.Size([752, 3, 2448])\n",
      "train_input_s.shape =  torch.Size([752, 3, 24])\n",
      "train_input_q.shape =  torch.Size([752, 3, 24])\n",
      "train_reg.shape =  torch.Size([752, 3, 24])\n",
      "train_y.shape =  torch.Size([752, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pred_len = 24\n",
    "batch = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "train_data[\"location_id\"] = train_data[\"location\"].apply(lambda x: loc_node2idx[x])\n",
    "test_data[\"location_id\"] = test_data[\"location\"].apply(lambda x: loc_node2idx[x])\n",
    "train_data[\"region_id\"] = train_data[\"region\"].apply(lambda x: reg_node2idx[x])\n",
    "test_data[\"region_id\"] = test_data[\"region\"].apply(lambda x: reg_node2idx[x])\n",
    "\n",
    "# train_data\n",
    "grouped = train_data.groupby(\"user\")\n",
    "train_X = pd.DataFrame(columns=[\"latitude\",\"longitude\",\"time_delta\",\n",
    "                                \"dist_delta\", \"location_id\",\"region_id\"])\n",
    "train_y = pd.DataFrame(columns=[\"latitude\",\"longitude\",\"time_delta\",\n",
    "                                \"dist_delta\", \"location_id\",\"region_id\"])\n",
    "for i,df in grouped:\n",
    "    df_user = df[[\"latitude\",\"longitude\",\"time_delta\", \"dist_delta\",\n",
    "                  \"location_id\",\"region_id\"]]\n",
    "    train_X = pd.concat([train_X, df_user.iloc[:pred_len,]])\n",
    "    train_y = pd.concat([train_y, df_user.iloc[pred_len:,]])\n",
    "## get data\n",
    "train_input_l = list(train_X[\"location_id\"])\n",
    "train_input_l = np.array([list(loc_embedding[i]) for i in train_input_l])\n",
    "train_coord = np.array(train_X[[\"latitude\", \"longitude\"]])\n",
    "train_input_l = np.concatenate((train_input_l,train_coord),axis=1).reshape(-1,pred_len*102)\n",
    "\n",
    "train_input_s = np.array(train_X[\"dist_delta\"]).reshape(-1,pred_len)\n",
    "train_input_q = np.array(train_X[\"time_delta\"]).reshape(-1,pred_len)\n",
    "# 归一化\n",
    "scaler_l = StandardScaler()\n",
    "scaler_s = StandardScaler()\n",
    "scaler_q = StandardScaler()\n",
    "train_input_l = scaler_l.fit_transform(train_input_l)\n",
    "print(train_input_l.shape)\n",
    "train_input_s = scaler_s.fit_transform(train_input_s)\n",
    "train_input_q = scaler_q.fit_transform(train_input_q)\n",
    "# reshape,set batch =3\n",
    "train_input_l = torch.from_numpy(train_input_l).half().reshape(-1,batch,pred_len*102).to(device)\n",
    "train_input_s = torch.from_numpy(train_input_s).half().reshape(-1,batch,pred_len).to(device)\n",
    "train_input_q = torch.from_numpy(train_input_q).half().reshape(-1,batch,pred_len).to(device)\n",
    "\n",
    "print(\"train_input_l.shape = \",train_input_l.shape)\n",
    "print(\"train_input_s.shape = \",train_input_s.shape)\n",
    "print(\"train_input_q.shape = \",train_input_q.shape)\n",
    "\n",
    "train_reg = np.array(train_X[\"region_id\"]).astype(float)\n",
    "train_reg = torch.from_numpy(train_reg).long().reshape(-1,batch,pred_len).to(device)\n",
    "print(\"train_reg.shape = \",train_reg.shape)\n",
    "\n",
    "train_y = np.array(train_y[\"location_id\"]).astype(float)\n",
    "train_y = torch.from_numpy(train_y).long().reshape(-1,batch, 4).to(device)\n",
    "print(\"train_y.shape = \",train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ab4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input_l.shape =  torch.Size([196, 3, 2448])\n",
      "test_input_s.shape =  torch.Size([196, 3, 24])\n",
      "test_input_q.shape =  torch.Size([196, 3, 24])\n",
      "test_y.shape =  torch.Size([196, 3, 4])\n",
      "test_reg.shape =  torch.Size([196, 3, 24])\n"
     ]
    }
   ],
   "source": [
    "# test_data\n",
    "# train_data\n",
    "grouped = test_data.groupby(\"user\")\n",
    "test_X = pd.DataFrame(columns=[\"latitude\",\"longitude\",\"time_delta\",\n",
    "                                \"dist_delta\", \"location_id\",\"region_id\"])\n",
    "test_y = pd.DataFrame(columns=[\"latitude\",\"longitude\",\"time_delta\",\n",
    "                                \"dist_delta\", \"location_id\",\"region_id\"])\n",
    "for i,df in grouped:\n",
    "    df_user = df[[\"latitude\",\"longitude\",\"time_delta\", \"dist_delta\",\n",
    "                  \"location_id\",\"region_id\"]]\n",
    "    test_X = pd.concat([test_X, df_user.iloc[:pred_len,]])\n",
    "    test_y = pd.concat([test_y, df_user.iloc[pred_len:,]])\n",
    "## get data\n",
    "test_input_l = list(test_X[\"location_id\"])\n",
    "test_input_l = np.array([list(loc_embedding[i]) for i in test_input_l])\n",
    "test_coord = np.array(test_X[[\"latitude\", \"longitude\"]])\n",
    "test_input_l = np.concatenate((test_input_l,test_coord),axis=1).reshape(-1,pred_len*102)\n",
    "\n",
    "test_input_s = np.array(test_X[\"dist_delta\"]).reshape(-1,pred_len)\n",
    "test_input_q = np.array(test_X[\"time_delta\"]).reshape(-1,pred_len)\n",
    "# 归一化\n",
    "test_input_l = scaler_l.transform(test_input_l)\n",
    "test_input_s = scaler_s.transform(test_input_s)\n",
    "test_input_q = scaler_q.transform(test_input_q)\n",
    "# reshape,set batch =3\n",
    "test_input_l = torch.from_numpy(test_input_l).half().reshape(-1,batch,pred_len*102).to(device)\n",
    "test_input_s = torch.from_numpy(test_input_s).half().reshape(-1,batch,pred_len).to(device)\n",
    "test_input_q = torch.from_numpy(test_input_q).half().reshape(-1,batch,pred_len).to(device)\n",
    "\n",
    "print(\"test_input_l.shape = \",test_input_l.shape)\n",
    "print(\"test_input_s.shape = \",test_input_s.shape)\n",
    "print(\"test_input_q.shape = \",test_input_q.shape)\n",
    "\n",
    "test_y = np.array(test_y[\"location_id\"]).astype(float)\n",
    "test_y = torch.from_numpy(test_y).long().reshape(-1,batch,4).to(device)\n",
    "print(\"test_y.shape = \",test_y.shape)\n",
    "\n",
    "test_reg = np.array(test_X[\"region_id\"]).astype(float)\n",
    "test_reg = torch.from_numpy(test_reg).long().reshape(-1,batch,pred_len).to(device)\n",
    "print(\"test_reg.shape = \",test_reg.shape)\n",
    "\n",
    "loc_embedding = torch.from_numpy(loc_embedding)\n",
    "loc_embedding = loc_embedding.half().to(device)\n",
    "reg_embedding = torch.from_numpy(reg_embedding)\n",
    "reg_embedding = reg_embedding.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecdee94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "Input_dim = pred_len*(POI_vec_dim+2)\n",
    "Hidden_1 = 480\n",
    "Hid_dim = 120\n",
    "N_layers = 3\n",
    "Pred_len = pred_len\n",
    "Dropout_E = 0.5\n",
    "Dropout_D = 0\n",
    "In_dim = POI_vec_dim\n",
    "Hid_d = 160\n",
    "Output_dim = num_POI\n",
    "In_feats = region_vec_dim\n",
    "Hidden_g = 96\n",
    "Out_g = 32\n",
    "\n",
    "encoder = Encoder(Input_dim, Hidden_1, Hid_dim, N_layers, Pred_len, Dropout_E)\n",
    "decoder = Decoder(In_dim, Hid_d, Output_dim, N_layers, Dropout_D)\n",
    "gcn_dist = GCN(In_feats, Hidden_g, Out_g, Hid_dim, N_layers, Pred_len).to(device)\n",
    "gcn_visit = GCN(In_feats, Hidden_g, Out_g, Hid_dim, N_layers, Pred_len).to(device)\n",
    "\n",
    "my_model = STMAGCN(encoder, decoder, gcn_dist, gcn_visit, device)\n",
    "my_model = my_model.to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=0.0001,weight_decay=0.0075)\n",
    "# 混合精度，放大梯度\n",
    "scaler = GradScaler()\n",
    "def backward(scaler, loss, optimizer):\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "seq_len, batch_size,_ = train_y.shape\n",
    "in_dim = decoder.in_dim\n",
    "input_0 = torch.zeros(seq_len, batch_size, in_dim).half()\n",
    "dist_graph = dist_graph.to(device)\n",
    "visit_graph = visit_graph.to(device)\n",
    "loss_dict_ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed7e5b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/400], Loss:9.6790628\n",
      "Epoch: [40/400], Loss:9.6760645\n",
      "Epoch: [60/400], Loss:9.6733446\n",
      "Epoch: [80/400], Loss:9.6703262\n",
      "Epoch: [100/400], Loss:9.6672068\n",
      "Epoch: [120/400], Loss:9.6633034\n",
      "Epoch: [140/400], Loss:9.6588001\n",
      "Epoch: [160/400], Loss:9.6528044\n",
      "Epoch: [180/400], Loss:9.6440229\n",
      "Epoch: [200/400], Loss:9.6285009\n",
      "Epoch: [220/400], Loss:9.5927601\n",
      "Epoch: [240/400], Loss:9.4836473\n",
      "Epoch: [260/400], Loss:9.2215309\n",
      "Epoch: [280/400], Loss:8.8715820\n",
      "Epoch: [300/400], Loss:8.6183281\n",
      "Epoch: [320/400], Loss:8.4525423\n",
      "Epoch: [340/400], Loss:8.3611631\n",
      "Epoch: [360/400], Loss:8.3102980\n",
      "Epoch: [380/400], Loss:8.2790728\n",
      "Epoch: [400/400], Loss:8.2578020\n",
      "399\n",
      "Wall time: 10min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_epochs = 400\n",
    "threshold = 4\n",
    "#my_model.load_state_dict(torch.load(path+'model_data.pkl'))\n",
    "for epoch in range(0, max_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    my_model.train()\n",
    "    with autocast(enabled=True):\n",
    "        outputs = my_model(poi_embedding=loc_embedding, input_l=train_input_l,\n",
    "                           input_s=train_input_s, input_q=train_input_q ,trg=train_y,\n",
    "                           input_0=input_0, dist_graph=dist_graph,visit_graph=visit_graph,\n",
    "                           inputs=train_reg,nodes_vec=reg_embedding)\n",
    "    # outputs : [4*seq_len*batch, output_dim]\n",
    "    # train_y : [seq_len, batch, 4]\n",
    "        loss = loss_function(outputs, train_y.reshape(-1,1).squeeze(dim=1))\n",
    "        loss_dict_[epoch] = loss.item()\n",
    "    backward(scaler, loss, optimizer)\n",
    "    optimizer.zero_grad()\n",
    "    if loss.item() < threshold:\n",
    "        print('Epoch [{}/{}], Loss: {:.7f}'.format(epoch+1, max_epochs, loss.item()))\n",
    "        print(\"The loss value is reached\")\n",
    "        break\n",
    "    elif (epoch+1) % 20 == 0:\n",
    "        print('Epoch: [{}/{}], Loss:{:.7f}'.format(epoch+1, max_epochs, loss.item()))\n",
    "\n",
    "torch.save(my_model.state_dict(), path+'model_data.pkl')\n",
    "print(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf8e96",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8427a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的Accuracy@k,k = 1, 5, 10, 20\n",
    "# 输入的pred为(n,cate)，label为(n,1)或(n)的ndarray\n",
    "def get_metrics(pred, label):\n",
    "    n,num_cate = pred.shape\n",
    "    acc_k = {}\n",
    "    MRR_k = {}\n",
    "    for k in [1,5,10,30]:\n",
    "        right_num = 0\n",
    "        mrr = 0\n",
    "        for i in range(n):\n",
    "            pred_ = pred[i]\n",
    "            order = np.argsort(pred_)[::-1]\n",
    "            order_k = order[:k]\n",
    "            if int(label[i]) in order_k:\n",
    "                right_num += 1\n",
    "            ranking = np.where(order == int(label[i]))[0]\n",
    "            mrr += 1/(int(ranking)+1)\n",
    "        str_k = \"acc@\"+str(k)\n",
    "        acc_k[str_k] = round(right_num/n,4)\n",
    "        str_k = \"MRR@\"+str(k)\n",
    "        MRR_k[str_k] = round(mrr/n,4)\n",
    "    return (acc_k, MRR_k)\n",
    "\n",
    "def get_POIi_metrics(pred, label):\n",
    "    acc_k_list = {}\n",
    "    MRR_k_list = {}\n",
    "    pred = pred.reshape(-1,3,num_POI*4)\n",
    "    label = label.reshape(-1,3,4)\n",
    "    index = [0, num_POI, num_POI*2, num_POI*3, num_POI*4]\n",
    "    for ith in range(4):\n",
    "        pred_ith = pred[:,:,index[ith]:index[ith+1]].reshape(-1,num_POI)\n",
    "        label_ith = label[:,:,ith].reshape(-1,1).squeeze(1)\n",
    "        str_i = \"POI-\"+str(ith)+\"metrics\"\n",
    "        acc_k_list[str_i] = get_metrics(pred_ith, label_ith)[0]\n",
    "        MRR_k_list[str_i] = get_metrics(pred_ith, label_ith)[1]\n",
    "    return (acc_k_list, MRR_k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24fc02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_train.shape =  (9024, 16054)\n",
      "train_y.shape =  (9024,)\n",
      "train set acc：\n",
      " ({'acc@1': 0.0369, 'acc@5': 0.0595, 'acc@10': 0.0757, 'acc@30': 0.1228}, {'MRR@1': 0.0527, 'MRR@5': 0.0527, 'MRR@10': 0.0527, 'MRR@30': 0.0527})\n",
      "train set acc of ith POI: ({'POI-0metrics': {'acc@1': 0.0142, 'acc@5': 0.0395, 'acc@10': 0.0576, 'acc@30': 0.1064}, 'POI-1metrics': {'acc@1': 0.0199, 'acc@5': 0.0488, 'acc@10': 0.0629, 'acc@30': 0.1095}, 'POI-2metrics': {'acc@1': 0.0355, 'acc@5': 0.0559, 'acc@10': 0.0723, 'acc@30': 0.1135}, 'POI-3metrics': {'acc@1': 0.078, 'acc@5': 0.094, 'acc@10': 0.1099, 'acc@30': 0.1618}}, {'POI-0metrics': {'MRR@1': 0.0315, 'MRR@5': 0.0315, 'MRR@10': 0.0315, 'MRR@30': 0.0315}, 'POI-1metrics': {'MRR@1': 0.0373, 'MRR@5': 0.0373, 'MRR@10': 0.0373, 'MRR@30': 0.0373}, 'POI-2metrics': {'MRR@1': 0.0506, 'MRR@5': 0.0506, 'MRR@10': 0.0506, 'MRR@30': 0.0506}, 'POI-3metrics': {'MRR@1': 0.0913, 'MRR@5': 0.0913, 'MRR@10': 0.0913, 'MRR@30': 0.0913}})\n",
      "out_test.shape =  (2352, 16054)\n",
      "test_y.shape =  (2352,)\n",
      "test set acc：\n",
      " ({'acc@1': 0.0391, 'acc@5': 0.0561, 'acc@10': 0.0693, 'acc@30': 0.1173}, {'MRR@1': 0.0518, 'MRR@5': 0.0518, 'MRR@10': 0.0518, 'MRR@30': 0.0518})\n",
      "test set acc of ith POI: ({'POI-0metrics': {'acc@1': 0.0238, 'acc@5': 0.0391, 'acc@10': 0.0612, 'acc@30': 0.1173}, 'POI-1metrics': {'acc@1': 0.0204, 'acc@5': 0.0391, 'acc@10': 0.0476, 'acc@30': 0.0901}, 'POI-2metrics': {'acc@1': 0.0391, 'acc@5': 0.0578, 'acc@10': 0.0646, 'acc@30': 0.102}, 'POI-3metrics': {'acc@1': 0.0731, 'acc@5': 0.0884, 'acc@10': 0.1037, 'acc@30': 0.1599}}, {'POI-0metrics': {'MRR@1': 0.0382, 'MRR@5': 0.0382, 'MRR@10': 0.0382, 'MRR@30': 0.0382}, 'POI-1metrics': {'MRR@1': 0.0332, 'MRR@5': 0.0332, 'MRR@10': 0.0332, 'MRR@30': 0.0332}, 'POI-2metrics': {'MRR@1': 0.05, 'MRR@5': 0.05, 'MRR@10': 0.05, 'MRR@30': 0.05}, 'POI-3metrics': {'MRR@1': 0.0859, 'MRR@5': 0.0859, 'MRR@10': 0.0859, 'MRR@30': 0.0859}})\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "my_model.train()\n",
    "\n",
    "with autocast(enabled=True):\n",
    "    out_train = my_model(poi_embedding=loc_embedding, input_l=train_input_l,\n",
    "           input_s=train_input_s, input_q=train_input_q ,trg=train_y,\n",
    "           input_0=input_0, dist_graph=dist_graph,visit_graph=visit_graph,\n",
    "           inputs=train_reg,nodes_vec=reg_embedding)\n",
    "# test\n",
    "my_model = my_model.eval()\n",
    "seq_len, batch_size,_ = test_y.shape\n",
    "in_dim = decoder.in_dim\n",
    "input_0 = torch.zeros(seq_len, batch_size, in_dim).half()\n",
    "with autocast(enabled=True):\n",
    "    out_test = my_model(poi_embedding=loc_embedding, input_l=test_input_l,\n",
    "           input_s=test_input_s, input_q=test_input_q ,trg=test_y,\n",
    "           input_0=input_0, dist_graph=dist_graph,visit_graph=visit_graph,\n",
    "           inputs=test_reg,nodes_vec=reg_embedding, teacher_forcing_ratio=-0.1)\n",
    "\n",
    "out_train = out_train.cuda().cpu().detach().numpy()\n",
    "train_y = train_y.reshape(-1,1).squeeze(dim=1)\n",
    "train_y = train_y.cuda().cpu().numpy()\n",
    "print(\"out_train.shape = \",out_train.shape)\n",
    "print(\"train_y.shape = \",train_y.shape)\n",
    "print(\"train set acc：\\n\",get_metrics(out_train, train_y))\n",
    "print(\"train set acc of ith POI:\",get_POIi_metrics(out_train, train_y))\n",
    "\n",
    "out_test = out_test.cuda().cpu().detach().numpy()\n",
    "test_y = test_y.reshape(-1,1).squeeze(dim=1)\n",
    "test_y = test_y.cuda().cpu().numpy()\n",
    "print(\"out_test.shape = \",out_test.shape)\n",
    "print(\"test_y.shape = \",test_y.shape)\n",
    "print(\"test set acc：\\n\",get_metrics(out_test, test_y))\n",
    "print(\"test set acc of ith POI:\",get_POIi_metrics(out_test, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e978f7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load previous loss\n",
    "#with open(path+'loss_dict_train.pickle', 'rb') as f:\n",
    "#    loss_dict_train = pickle.load(f)\n",
    "\n",
    "# get loss dict\n",
    "loss_dict_train = {}\n",
    "loss_dict_train.update(loss_dict_)\n",
    "# save loss dict\n",
    "loss_dict_train_file = open(path+'loss_dict_train.pickle', 'wb')\n",
    "pickle.dump(loss_dict_train, loss_dict_train_file)\n",
    "loss_dict_train_file.close()\n",
    "# paint\n",
    "plt.rcParams['font.sans-serif']=['SimHei']  #解决中文显示乱码问题\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(list(loss_dict_train.keys()), list(loss_dict_train.values()),\n",
    "        marker='.', label = \"train_loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ee50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
